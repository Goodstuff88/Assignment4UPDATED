---
title: "Assignment 4"
author: "Gustav Oosthuizen - OSTGUS001"
date: '`r format(Sys.Date(), "%d-%B-%Y")`'
output:
  pdf_document:
    highlight: pygments
    keep_tex: yes
    number_sections: yes
    toc: yes
    toc_depth: 4
    fig_caption: true
citation_package: "biblatex"
bibliography: "Ass_4.bib"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\pagebreak

```{r installing packages, include=FALSE}

#Steps:

# 1. Installing packages used for analysis :

#install.packages("tidyverse")
#install.packages("remotes")
#install.packages("maps")
#install.packages("geojsonio")
#install.packages("rgdal")
#install.packages("spdplyr")
#install.packages("WDI")
#install.packages("sf")
#install.packages("countrycode")
#install.packages("revgeo")
#install.packages("ggmap")
#install.packages("rnaturalearth")
#install.packages("rnaturalearthdata")
install.packages("gganimate")
install.packages("mapdata")
install.packages("devtools")
install.packages("Rtools")
devtools::install_github("thomasp85/gganimate/releases/tag/v0.1.1", force = TRUE)

library(Rtools)
library(devtools)
library(tidyverse)
library(remotes)
library(plotly)
library(leaflet)
library(maps)
library(geojsonio)
library(rgdal)
library(spdplyr)
library(WDI)
library(sf)
library(countrycode)
library(revgeo)
library(ggmap)
library(rnaturalearth)
library(rnaturalearthdata)
library(rnaturalearthhires)
library(gganimate)
library(mapdata)
# 2. Installing packages used to get corona virus data set

#remotes::install_github('pzhaonet/ncovr')
#install.packages("ncovr")
#library(ncovr)





```

**** Still Need to check this code - dowloads the latest version of the Corona Virus ****

```{r Loadinging and transforming most current data, include=FALSE}
#### Does not WORk

# Helper file to download the latest coronavirus data using the ncovr R package 
# see https://github.com/pzhaonet/ncovr for more details.

### ONLY DO THIS ONCE
# you need the remotes packages for this, install if you don't have
# install.packages("remotes")
# get the ncovr package
remotes::install_github('pzhaonet/ncovr')
###

### Get the latest data

# load required packages
library(ncovr)

# get the latest data
ncov <- get_ncov() 

# extract the area data frame, which contains everything we need
ncov <- ncov$area

# a function for converting Julian time to ymd_
conv_time <- function(x){
  as.POSIXct('1970-01-01', tz = 'GMT') + x / 1000
}

# convert updateTime from Julian date to ymd-hms
ncov$updateTime <- conv_time(ncov$updateTime)

save(ncov, file = "ncov-latest.Rdata")

```

\section{Data wrangling} \label{DW section}

The data wrangling consist of importing, tidying and transforming the data set [@wickham2016r].

\subsection{Imporing data} 

As mentioned in section \ref{DW section} the first step of data wrangling is importing the data.
```{r importing data, echo=FALSE}

# Importing the data set "ncov"

load("C:/Users/Gustav/Desktop/MSc Data Science 1st year/Exploratory data analysis/Assignments/Assignment4/ncov-latest (1).Rdata")

head(ncov, 5)

```  

\subsection{Tiding and transforming the data}

The second and thrid steps of data wrangling is tiding and transforming the data. The variables of the "ncovr" data set will be investigated and the nesseccary changes/transformations will be made to get the data in a tidy format.   

```{r "ncovr" variable investigation, include=FALSE}

# Note: The following code was commented out, since it was just included to show the investigation process. No changes were made to the data set. 

# Steps:

# 1. General structure of the data set:

# glimpse(ncov)
 
# structure(ncov)

# 2. Variables that seem to have no value and should be removed.

# provinceName: Chinese writings - no use
# provinceShortName: Chinese writings - no use
# comment: Chinese writings - no use
# statisticsData: Contains a lot of NA's. Cant see the use in variable that gives link.
# cities: Contains a lot of NULL values that are useless.
# countryName: Chinese writings - no use
# continentName:Chinese writings - no use
# operator: No use for assignment
# countryShortCode: Contains a lot of NA's. Would Rather remove.
# createTime: Contains a lot of NA's and dont know meaning
# modifyTime: Contains a lot of NA's and dont know meaning 
# cityName: Contains no names and NA's
# province_en: Contains no names and NA's 

#3.1 Investigating "comment" variable:

# ncov %>%
#   filter(comment != "") %>%
#   count()

# Conclution: Most rows (7598) have no comments in them and it 

#3.2 investigating "location.ID" variable:

# sum(is.na(ncov$locationId))

# Conclution: No missing values, can be useful.

#3.3 Investigating "statisticsData" variable:

# sum(is.na(ncov$statisticsData))

# ncov %>%
#   filter(!is.na(ncov$statisticsData))

# Conclution:Contains a lot of NA values

#3.4 Investigating "cities" variable

# view first data frame in list

# ncov$cities[1]

# Conclution: Contains a lot of NULL values. Values that are not NUll are data frames that seem to have similar but less data of than the original data set. 

#3.5 Investigating "operator" variable

# sum(is.na(ncov$operator))

# ncov %>%
#   filter(!is.na(ncov$operator))
  
# Conclution: Don't see the use of including variable for the purpose of assignment.

#3.6 Investigating "countryShortCode" variable

# sum(is.na(ncov$countryShortCode))

# Conclution: Contains a lot of NA's. Would Rather remove.

# 3.6 Investigating "province_en" variable 

# sum(is.na(ncov[,23]))

# ncov %>%
#   filter(!is.na(province_en))

# Conclution: Contains a lot of NA's. Would Rather remove

```

***** START CODE - for reproduceability *****

```{r Removing unwanted variables from ncov, echo=FALSE}

# Selecting only the most important variables from "ncov" data set 

ncov_Updated <- ncov %>%
  select(-c("provinceName", "provinceShortName", "comment", "statisticsData", "cities", "countryName", "continentName","operator", "countryShortCode", "createTime", "modifyTime", "cityName", "province_en" ))

head(ncov_Updated,5)
```

```{r confirming data values}

# According to the author the following should be true:

# confirmedCount = currentConfirmedCount + curedCount + deadCount

# An description of the variables are as follows:

# 1. confirmedCount: The amount of people that has/had the corona virus.
# 2. currentConfirmedCount: The number of people whom currenly (given the time and area) has the corona virus.
# 3. curedCount: The amount of people who had the corona virus.
# 4. deadCount: The number of people who died from the corona virus.

# But in some cases the currentConfirmedCount has a N/A value associated with it. This can mean that the value is uncertain. But since the other values of the expression is know, the value can be calculated in the following steps.

# Steps:

# 1. Creating new variable "currentConfirmedCount_Complete" and removing o

ncov_Updated <-  ncov_Updated %>%
  mutate(currentConfirmedCount_Complete = confirmedCount - curedCount - deadCount)

# 2. Checking that the "currentConfirmedCount_Complete" equals the "currentConfirmedCount" for all cases where "currentConfirmedCount" is not NA (testing pupose).

# ncov_Updated %>%
#   select(currentConfirmedCount, currentConfirmedCount_Complete) %>%
#   filter(!is.na(currentConfirmedCount)) %>%
#   filter(currentConfirmedCount != currentConfirmedCount_Complete)

# Therfore we can conclude that the way of estimating currentConfirmedCount_Complete is correct

# 3. Removing the currentConfirmedCount

ncov_Updated <- ncov_Updated %>%
  select(-c("currentConfirmedCount"))

# 4. Since "suspectedCount" + "curedCount" + "deadCount" does not = "confirmedCount", it seems that the variable has a lot of uncertainty attached to it. Therefore it will be excluded from the analysis.

ncov_Updated <- ncov_Updated %>%
  select(-c("suspectedCount"))

# 5. Renaming and rearranging some of the variables

ncov_Updated <- ncov_Updated %>%
  rename( Country_Name = countryEnglishName,
          Continent_Name = continentEnglishName,
          Province_Name = provinceEnglishName,
          Location_ID = locationId)

ncov_Updated <- ncov_Updated[ ,c(9, 2, 3, 1, 4, 6, 5, 7, 8)] 

```



```{r Final checking for missing values}

# Steps:

# 1. Checking which variables in data set contains missing values

sum(is.na(ncov_Updated$currentConfirmedCount_Complete))
sum(is.na(ncov_Updated$curedCount))
sum(is.na(ncov_Updated$deadCount))
sum(is.na(ncov_Updated$confirmedCount))
sum(is.na(ncov_Updated$Location_ID))
sum(is.na(ncov_Updated$Continent_Name))
sum(is.na(ncov_Updated$Country_Name))
sum(is.na(ncov_Updated$Province_Name))
sum(is.na(ncov_Updated$updateTime))

# "Continent_Name", "Country_Name" and "Province_Name" seems to have missing vaues which will be probalmatic when trying to merge it with spatial data. 

# 2. Investigating the "Province_Name" variable (Testing pupose)

# ncov_Updated %>% 
#   select(Continent_Name, Country_Name, Province_Name )

# Note that only most chinese provinces are listed but NOT other countries for example Newzealand province is Newzealand. This will not help in further analysis. Therefore the "Province_Name" variable will be dropped.

# 3. Excluding "Province_Name" variable

ncov_Updated <- ncov_Updated %>% 
  select(-Province_Name)

# The following code chunks will try and populate values for the NA instances in the "Continet_Name" and "Country_Name" variable.

```

Correcting NA's found in *Continent_Name*

```{r Correcting missing values found in "Continent_Name"}

# By using the data provided, the continent names can that have NA instances can be removed.

# Steps: 

# 1.  Creating  completed data frame with the countries and their associated continents

Continent_Country_df <- ncov_Updated %>%
  group_by(Continent_Name, Country_Name) %>%
  summarise(count = n()) %>%
  na.omit() %>%
  select(Country_Name, Continent_Name) %>%
  as.data.frame()

Na_Entry <- data.frame(Country_Name =  NA,
                       Continent_Name = NA)

Continent_Country_df <-  rbind(Continent_Country_df, Na_Entry) %>%
  rename(Continent_NEW_Name = Continent_Name)


# 2. Using the created data frame to fill in for the missing values of the original data frame.


ncov_Updated <-  left_join(ncov_Updated, Continent_Country_df, by = "Country_Name" )

# Note: The missing values in "Continent_Name"  previously encoutered are gone, but there are still missing values in the new variable "Continen_NEW_name". This is Because the two data frames were joined by the variables "Country_Name" which also contained some missing values.   

# 3. By using the original "Country_Name" and the new "Country_NEW_Name" the missing values in Country_NEW_Name can be completely removed. This is done by the following for loop:  

for(i in 1:nrow(ncov_Updated)) {
  
  if (is.na(ncov_Updated$Continent_NEW_Name[i])) {
    ncov_Updated$Continent_NEW_Name[i] = ncov_Updated$Continent_Name[i] 
  }
  
}
  
# 4. Checking for missing values in the new complete variable "Continent_NEW_Name"

ncov_Updated %>%
 filter(is.na(Continent_NEW_Name))

# Note: As seen by the filter result there are only two obseravations that report missing values. 

# 5. Filtering by "location_ID" to see if the values are uniques or if they have Continents or countries associated with them

ncov_Updated %>%
filter(Location_ID == 955019)

# Note: Asia is associated with the values

# 7. Final updating of values

ncov_Updated$Continent_NEW_Name[ncov_Updated$Location_ID == 955019] <- "Asia"

# 8. Removing old "Continent_Name" 

ncov_Updated <- ncov_Updated %>%
  select(-Continent_Name)


```

Correcting NA's found in *Country_Name*

``` {r Correcting missing values found in "Country_Name" variable}

# Steps

# 1. Checking wich codes do not have countries associated with them

ncov_Updated %>%
  filter(is.na(Country_Name)) %>%
  group_by(Location_ID)  %>%
  summarise(Count = n())

# Note: only 5 codes do not have country names (i.e.NA's)

# 2. Investigating the country codes with no country names (Testing purpose)

# Note: The original data set (ncov) was used because it was realised that there where chinese names associated with the codes where the engilsh country names were unknown. Google translate was used to determine the english country names of the 5 location_Id.   

# 2.1 location_ID = 955019

# Country_55019 <- ncov %>%
#   filter(locationId == 955019)

# Conclution: counrty is United Arab Emirates (Asia)

# 2.2 location_ID = 965002

# Country_965002 <- ncov %>%
#   filter(locationId == 965002)

# Conclution: Country is Andorra (Europe)

# 2.3 location_ID = 965005 

# Country_965005 <- ncov %>%
#   filter(locationId == 965005)

# Conclution: Country is Croatia (Europe)

# 2.4 location_ID = 974006 

# Country_974006 <- ncov %>%
#   filter(locationId == 974006)

# Conclution: Country is Dominica (North America)

# 2.5 location_ID = 0

# There are two "countries" using the code 0  - Europe and Others

# 2.5.1) 
# Country_0_Europe <- ncov %>%
# filter(locationId == 0 & continentEnglishName == "Europe")

# Conclution: country is North Macedonia (Europe)

# 2.5.2) 
# Country_0_Cruise <- ncov %>%
#   filter(locationId == 0 & continentEnglishName == "Others")

# Diamond Princess Cruises - not really a country (Others)

# 3) Populating the countries that NA values according to their code

ncov_Updated$Country_Name[ncov_Updated$Location_ID == 955019] <- "United Arab Emirates"

ncov_Updated$Country_Name[ncov_Updated$Location_ID == 965002] <- "Andorra"

ncov_Updated$Country_Name[ncov_Updated$Location_ID == 965005] <- "Croatia"

ncov_Updated$Country_Name[ncov_Updated$Location_ID == 974006] <- "Dominica"

ncov_Updated$Country_Name[ncov_Updated$Location_ID == 0 & ncov_Updated$Continent_NEW_Name == "Europe"] <- "North Macedonia"

ncov_Updated$Country_Name[ncov_Updated$Location_ID == 0 & ncov_Updated$Continent_NEW_Name == "Others"] <- "Other"

```

```{r Renaming and rearanging the complete columns}

# Renaming the completed continent and country variables 

ncov_Updated <- ncov_Updated %>%
  rename(Continent_Name_Comp = Continent_NEW_Name,
         Country_Name_Comp = Country_Name)

ncov_Updated <- ncov_Updated[ ,c(1, 2, 3, 4, 5, 8, 6, 7)]

```

Merging the "ncov_Updated" data set with a spatial data set.

```{r leaflet testing}

# 1. Adding world spacial information 
World_Countries <- geojson_read("countries.geojson", what = "sp")

World_countries_df <-  World_Countries@data

leaflet(World_Countries)  %>%
  #addProviderTiles(providers$Esri.WorldGrayCanvas) %>%
  addPolygons(stroke = FALSE, color = "White", weight = "1", smoothFactor = 0.3,
               fillOpacity = 0.7, fillColor = "lightblue")

# 2. Does not work 

World_countries_2 <- readOGR(dsn ="C:/Users/Gustav/Desktop/MSc Data Science 1st year/Exploratory data analysis/Assignments/Assignment4", layer = "gadm36", verbose = FALSE)


rm(World_countries_2)

# 3. .

Continents_Data <-  readOGR(dsn = "C:/Users/Gustav/Desktop/MSc Data Science 1st year/Exploratory data analysis/Assignments/Assignment4", layer = "continents", verbose = FALSE )
 
Continents_df <- Continents_Data@data


```

```{r Getting country data from world bank data }

#steos

# 1. Searchin WDI data base

WDIsearch(string = "GDP",
          field = "name",
          short = FALSE)

# 2. Retriving data

WDI_df <- data.frame( 
  WDI(country = "all",
  start = 2018,
  end = 2018,
  extra = TRUE
  )
)

# Extracting only useful infromation from dataframe

Complete_Country_Continent_df <-  WDI_df %>%
  select(iso3c, country, region) %>%
  filter(region != "Aggregates") %>%
  group_by(region) %>%
  summarise(count = n())


```

***** NEW DATA SET *****

```{r Loading new ncov data}


# Loading latest data (not going to be used for analysis)

# ncov_new <- read.csv("https://raw.githubusercontent.com/datasets/covid-19/master/data/time-series-19-covid-combined.csv")

# Loading the data up to the 23/03/2020

# For reproduceability

rm(ncov_new)

ncov_new <- read.csv(file = "time_series_19_covid_combined_23_03_2020.csv", stringsAsFactors = FALSE)


```

```{r Investigating variables in data sets}

# Steps

# 1. Investigating type and structre of data

glimpse(ncov_new)

str(ncov_new)

# Remark: The "Date" variable should of type date not factor:


# 2. Checking for NA values in data set

sum(is.na(ncov_new$Province.State))
sum(is.na(ncov_new$Country.Region))
sum(is.na(ncov_new$Lat))
sum(is.na(ncov_new$Long))
sum(is.na(ncov_new$Date))
sum(is.na(ncov_new$Confirmed))
sum(is.na(ncov_new$Recovered))
sum(is.na(ncov_new$Deaths))

# Remark: Seems as if there is only missing values in the "Recovered" variable

# 3. Looking at missing value instances

ncov_new %>% 
  filter(is.na(Recovered))

# Replace NA values with 0

ncov_new$Recovered[is.na(ncov_new$Recovered)] <- 0

ncov_new %>% 
  filter(is.na(Recovered))

```

```{r additions and transformations to data set}

# 1. Changing the format of the "Date" variable

ncov_new$Date <- as.Date(ncov_new$Date,)

# 2. Adding a variable "Infected" which is the number of accounts of people that are still undergoing treatment.

ncov_new <- ncov_new %>% 
  mutate(Infected = Confirmed - Recovered - Deaths)


# 3. Renaming some of the variables to make it more intuitive

ncov_new <- ncov_new %>% 
  rename(Cum_Confirmed = Confirmed,
         Province_State = Province.State,
         Country_Region = Country.Region)

```

```{r Loading spatial data and transfrorming to geojson format}

# Steps:

# 1. Reading in shape file dowloaded from  the natural earth data - http://www.naturalearthdata.com/downloads/

Spatial_df <- readOGR(dsn = "C:/Users/Gustav/Desktop/MSc Data Science 1st year/Exploratory data analysis/Assignments/Assignment4", layer =  "ne_10m_admin_1_states_provinces", verbose = FALSE )

# 2. Converting SP data frame to geojson format

Spatial_df_geojson <- geojson_json(Spatial_df)

# Saving the geojson file to local file library

geojson_write(Spatial_df_geojson, file = "C:/Users/Gustav/Desktop/MSc Data Science 1st year/Exploratory data analysis/Assignments/Assignment4/Countries_States.geojson")

```

```{r joing data frames}


# The following code will join the data of the ncov_new data frame with the Spatial data frame

# Steps


# 1. Look at the data of the spacial data (To do test merge)

Spatial_data_df <- Spatial_df@data

# 2. Decide on variables that seem important in spatial data frame

# Variables that seem important are:

# 1. "name": seems to be the province/area/State name (just for add. info)

# 2. "type_en" : Seems to describe the type of region (just for add. info)

# 3. "mapcolor9" : Seems to colour the regions maybe by continents

# 4. "mapcolor13": Seems to colour the regions maybe by continents

# 5. "latitude": Latitude of location (NB)

# 6. "longitude": Longitude of location (NB)

# 7. "admin": Seems to be the countries name (NB)

ncov_Updated

Test_df <- revgeo(latitude = -32.0275, longitude = -59.2824, provider = "google", item = "country", API = "AIzaSyDAlBCwDDrYdTD2OT17X8BjwvlmwxG_PdU")


filter(ncov_new, Country_Region == "Thailand"  )

filter(Spatial_data_df, admin == "Thailand")

# Revese geocoding


     

```

```{r Tut - Spatial mapping in R}

# 1. Basics

# 1.1 Loading the world data

World <- ne_countries(scale = "medium", returnclass = "sf")

class(World)

# 1.2 Basic plot of data

ggplot(data = World) +
    geom_sf(color = "black", fill = "lightblue" )

# Note: color refers to line color and fill to the fill of the polygons

st_crs(World)

# 1.3 Labeling the map

World_points <- st_centroid(World)
World_points <- cbind(World, st_coordinates(st_centroid(World$geometry)))

ggplot(data = World) +
  geom_sf() +
  geom_text(data = World_points, aes(x=X, y=Y, label=name),
   color = "darkblue", fontface = "bold", check_overlap = FALSE) +
  annotate(geom = "text", x = -90, y = 26, label = "Gulf of Mexico", 
   fontface = "italic", color = "grey22", size = 6) +
  coord_sf(xlim = c(-102.15, -74.12), ylim = c(7.65, 33.97), expand = FALSE)

# 2. Additional layers

```


```{r Merging of Spatial data and ncov_new data}

# Note: The "Country_Region" variable will be used to join the spatial data frame (obtained by the rnaturalearth packages) and the ncov_new data frame. Therfore, additional features will be added to the geometries.  

# Steps

# 1. Testing to see which country names do NOT coincide between the two data frames.

# 1.1 Creating a data frame (World_df) that list all the countries of the natural earth data.

World_df <- data.frame( Country_Region = World$name_long,
                        Pop_Estimate = World$pop_est
)


# 1.2 Joining the ncov_new data frame with the World_df date frame to investigate cases where Pop_Estimate is NA (i.e there is a mis match in country names and the join cannot be complete)


Country_Test_df <- left_join(ncov_new, World_df, by = "Country_Region" )

sum(is.na(Country_Test_df$Pop_Estimate))

# Remark: There is NA present. These country names that cause this should be investigated

# 1.3 Investigation of the cuntries that present NA values

 Mismatch_Countries <- filter(Country_Test_df, is.na(Pop_Estimate)) %>%
  group_by(Country_Region) %>%
  summarise(count = n())

 # Remark: Luckily, there is only 15 counties that does not cooincide. These countries will be manually renamed so that they can be merged successfully.  
 
# 1.4  Change the following names of the ncov_new data set.

# 1. Brunei -> Brunei Darussalam 
# 2. Congo (Brazzaville) -> Republic of the Congo
# 3. Congo (Kinshasa) -> Democratic Republic of the Congo
# 4. Cote d'Ivoire -> Côte d'Ivoire 
# 5. Cruise Ship -> Japan 
# 6. Czechia -> Czech Republic 
# 7. Eswatini -> eSwati
# 8. Gambia, The ->The Gambia
# 9. Holy See -> Vatican 
# 10. Korea, South -> Republic of Korea 
# 11. North Macedonia -> Macedonia 
# 12. Russia -> Russian Federation 
# 13. Taiwan* -> Taiwan 
# 14. US -> United States 
# 15. Cabo Verde -> Republic of Cabo Verde 

ncov_new$Country_Region[ncov_new$Country_Region == "Brunei"] <- "Brunei Darussalam"

ncov_new$Country_Region[ncov_new$Country_Region == "Congo (Brazzaville)"] <- "Republic of the Congo"

ncov_new$Country_Region[ncov_new$Country_Region == "Congo (Kinshasa)"] <- "Democratic Republic of the Congo"

ncov_new$Country_Region[ncov_new$Country_Region == "Cote d'Ivoire"] <- "Côte d'Ivoire"

ncov_new$Country_Region[ncov_new$Country_Region == "Cruise Ship"] <- "Japan"

ncov_new$Country_Region[ncov_new$Country_Region == "Czechia"] <- "Czech Republic"

ncov_new$Country_Region[ncov_new$Country_Region == "Eswatini"] <- "eSwatini"

ncov_new$Country_Region[ncov_new$Country_Region == "Gambia"] <- "The Gambia"

ncov_new$Country_Region[ncov_new$Country_Region == "Holy See"] <- "Vatican"

ncov_new$Country_Region[ncov_new$Country_Region == "Korea, South"] <- "Republic of Korea"

ncov_new$Country_Region[ncov_new$Country_Region == "Russia"] <- "Russian Federation"

ncov_new$Country_Region[ncov_new$Country_Region == "US"] <- "United States"

ncov_new$Country_Region[ncov_new$Country_Region == "Cabo Verde"] <- "Republic of Cabo Verde"

ncov_new$Country_Region[ncov_new$Country_Region == "North Macedonia"] <- "Macedonia"

ncov_new$Country_Region[ncov_new$Country_Region == "Taiwan*"] <- "Taiwan" 


# 1.5 Re-joining the Country_Test_df with changed country names

Country_Test_df <- left_join(ncov_new, World_df, by = "Country_Region" )

sum(is.na(Country_Test_df$Pop_Estimate))


# Remark: There are no missing values, therefore ALL the contries names of the ncov_new data set and the spatial data set coincides and the ACTUAL merging of features can continue.


# 2. Actual merging of spatial data (World) and ncov_new

# 2.1 Selcting only important features from the world data set and storing in World_Merge_df

World_Merge_df <- World %>% 
  select(name_long, iso_a3) %>%
  rename(Country_Region  = name_long)

# 2.2 Final megrge 

ncov_new_Spatial <- left_join(World_Merge_df, ncov_new, by = "Country_Region")

class(ncov_new_Spatial)

# 2.3 Changing the class of the new created df to be a data frame as well as sf

st_geometry(ncov_new_Spatial) <- ncov_new_Spatial$geometry

class(ncov_new_Spatial)

```

```{r additional changes to new created spatial data frame}

# The goal of this section is to remove and change some variables of the ncov_new_Spatial data frame


# Steps:


# 1. Remove "Province_State" variable (since it will not be used in the analysis)

ncov_new_Spatial <- ncov_new_Spatial %>%
  select()


glimpse(ncov_new_Spatial)


attr(ncov_new_Spatial, "sf_column")


```



```{r FINAL Spatial data Merging operation}

# The following code chunk deals with creating a data frame that contains the nessecceray data that will be used to create plots on a wolrd map.


# Steps:

# 1. Creating the data frame that encapsulates all the data needed for creating animation. The data frame basically gives a count summary of each country in the data set per day (from 2020-01-22 to 2020-03-23) 


ncov_new_FINAL_data <-  ncov_new %>% 
  group_by(Date, Country_Region) %>% 
  summarise(Cum_Confirmed_Country = sum(Cum_Confirmed),
            Recovered_Country = sum(Recovered),
            Deaths_Country = sum(Deaths),
            Infected_Country = sum(Infected)) %>% 
  as.data.frame()

class(ncov_new_FINAL_data)

# 1.1 Changing Some variables that are type double to integer
 
ncov_new_FINAL_data$Recovered_Country <- as.integer(ncov_new_FINAL_data$Recovered_Country)
ncov_new_FINAL_data$Infected_Country <- as.integer(ncov_new_FINAL_data$Infected_Country)


# 2. Merging of data with Spatial data from the natureal earth packages

ncov_new_Spatial <- left_join(ncov_new_FINAL_data, World_Merge_df , by = "Country_Region")

# 2.1 Classifying the geometry feature in data frame

st_geometry(ncov_new_Spatial) <- ncov_new_Spatial$geometry



```

```{r plot testing}




Test_df <- ncov_new_Spatial %>% 
  filter(Date == "2020-03-23")


 Map <- ggplot()  +
   geom_sf(data = ncov_new_Spatial, aes(frame = Date, fill = Cum_Confirmed_Country)) + 
   coord_sf(xlim = c(-30, 55), ylim = c(-40, 40), expand = FALSE) +
    scale_fill_gradient(low="blue", high="red", trans = "log10")
   
 
Map

ggplotly(Map)
 
 Map + transition_manual(Date)  +
  labs(title = "Date: {current_frame}")
 

animate(Map)

Map$labels


```






```{r leaflet testing with new data}

Spatial_df <- readOGR("Countries_States.geojson")


leaflet(ncov_new)  %>%
  addProviderTiles(providers$Esri.NatGeoWorldMap) %>%
  addMarkers(lat = 35.4437	 , lng = 139.638 )


```


```{r Using R maps}

Map_World <- map_data("world2Hires")

ggplot() + 
  geom_polygon(data = Map_World, aes(x = long, y = lat, group = group ))









```




\section{Visualisations}

As seen by figure \ref{fig:Normal_Distribution}, the normal distribution is plotted.
Remember the default figure width height is 6 x 4.5 

```{r Normal_Distribution, fig.height= 3, fig.width=4.5 , fig.align='center', fig.cap= "The normal distribution curve", echo=FALSE}

x <- rnorm(100)

hist(x)


```


\pagebreak

\section{References}

